<?xml version="1.0"?>
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
        <description>Block replication</description>
    </property>

    <property>
        <name>dfs.block.size</name>
        <value>10485760</value>
        <description>Want more mappers for in-mem cubing, thus smaller the DFS block size</description>
    </property>

    <property>
        <name>hive.exec.compress.output</name>
        <value>true</value>
        <description>enable compress</description>
    </property>

    <property>
        <name>hive.auto.convert.join.noconditionaltask</name>
        <value>true</value>
        <description>enable map-side join</description>
    </property>

    <property>
        <name>hive.auto.convert.join.noconditionaltask.size</name>
        <value>300000000</value>
        <description>enable map-side join</description>
    </property>

    <!--
    <property>
        <name>mapreduce.map.output.compress.codec</name>
        <value>com.hadoop.compression.lzo.LzoCodec</value>
        <description></description>
    </property>
    <property>
        <name>mapreduce.output.fileoutputformat.compress.codec</name>
        <value>com.hadoop.compression.lzo.LzoCodec</value>
        <description></description>
    </property>
    -->
    <property>
        <name>hive.merge.mapfiles</name>
        <value>true</value>
        <description>Enable hive file merge on mapper only job</description>
    </property>
    <property>
        <name>hive.merge.mapredfiles</name>
        <value>true</value>
        <description>Enable hive file merge on map-reduce job</description>
    </property>
    <property>
        <name>mapred.output.compression.type</name>
        <value>BLOCK</value>
        <description>The compression type to use for job outputs</description>
    </property>
    <property>
        <name>hive.merge.size.per.task</name>
        <value>256000000</value>
        <description>Size for the merged file</description>
    </property>
</configuration>